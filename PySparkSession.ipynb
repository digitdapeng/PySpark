{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\n",
    "             (4, 5, 6, 'd e f'),\n",
    "             (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|null|   5|   6|d e f|\n",
      "|   7|null|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData = spark.sparkContext.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])\n",
    "print(myData.collect())\n",
    "type(myData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------------+\n",
      "| Id| Name|Sallary|DepartmentId|\n",
      "+---+-----+-------+------------+\n",
      "|  1|  Joe|  70000|           1|\n",
      "|  2|Henry|  80000|           2|\n",
      "|  3|  Sam|  60000|           2|\n",
      "|  4|  Max|  90000|           1|\n",
      "+---+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By using createDataFrame( ) function\n",
    "Employee = spark.createDataFrame([\n",
    "                        ('1', 'Joe',   '70000', '1'),\n",
    "                        ('2', 'Henry', '80000', '2'),\n",
    "                        ('3', 'Sam',   '60000', '2'),\n",
    "                        ('4', 'Max',   '90000', '1')],\n",
    "                        ['Id', 'Name', 'Sallary','DepartmentId']\n",
    "                       )\n",
    "Employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- TV: string (nullable = true)\n",
      " |-- Radio: string (nullable = true)\n",
      " |-- Newspaper: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By using read and load functions,  or using format('csv')\n",
    "# from csv\n",
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                load(\"Advertising.csv\",header=True)\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset from DataBase\n",
    "## User information\n",
    "user = 'your_username'\n",
    "pw   = 'your_password'\n",
    "\n",
    "## Database information\n",
    "table_name = 'table_name'\n",
    "url = 'jdbc:postgresql://##.###.###.##:5432/dataset?user='+user+'&password='+pw\n",
    "properties ={'driver': 'org.postgresql.Driver', 'password': pw,'user': user}\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset from HDFS\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc= SparkContext('local','example')\n",
    "hc = HiveContext(sc)\n",
    "tf1 = sc.textFile(\"hdfs://cdhstltest/user/data/demo.CSV\")\n",
    "print(tf1.first())\n",
    "\n",
    "hc.sql(\"use intg_cme_w\")\n",
    "spf = hc.sql(\"SELECT * FROM spf LIMIT 100\")\n",
    "print(spf.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Operations\n",
    "### transformation and actions please see contect notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "0  a  1  2\n",
      "1  b  2  3\n",
      "2  c  3  4\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  1|  2|\n",
      "|  b|  2|  3|\n",
      "|  c|  3|  4|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "my_list = [['a', 1, 2], ['b', 2, 3],['c', 3, 4]]\n",
    "col_name = ['A', 'B', 'C']\n",
    "# caution for the columns=\n",
    "print(pd.DataFrame(my_list,columns= col_name).head())\n",
    "spark.createDataFrame(my_list, col_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C\n",
      "0  0  1  1\n",
      "1  1  0  0\n",
      "2  0  1  0\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  0|  1|  1|\n",
      "|  1|  0|  0|\n",
      "|  0|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from dictionary\n",
    "d = {'A': [0, 1, 0],\n",
    "     'B': [1, 0, 1],\n",
    "     'C': [1, 0, 0]}\n",
    "dp=pd.DataFrame(d)\n",
    "print(pd_df.head())\n",
    "# Tedious for PySpark\n",
    "ds=spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys()))\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from json\n",
    "dp = pd.read_json(\"data/data.json\")\n",
    "ds = spark.read.json('data/data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|     A|  B|   C|\n",
      "+------+---+----+\n",
      "|  male|  1|null|\n",
      "|female|  2|   3|\n",
      "|  male|  3|   4|\n",
      "+------+---+----+\n",
      "\n",
      "+---+---+----+\n",
      "|  A|  B|   C|\n",
      "+---+---+----+\n",
      "|  1|  1|null|\n",
      "|  0|  2|   3|\n",
      "|  1|  3|   4|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe operation\n",
    "my_list = [['male', 1, None], ['female', 2, 3],['male', 3, 4]]\n",
    "dp = pd.DataFrame(my_list,columns=['A', 'B', 'C'])\n",
    "ds = spark.createDataFrame(my_list, ['A', 'B', 'C'])\n",
    "ds.columns\n",
    "ds.show()\n",
    "ds.dtypes\n",
    "ds.fillna(-99)\n",
    "ds.na.replace(['male','female'],['1','0']).show()  #replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|     A|  B|  CC|\n",
      "+------+---+----+\n",
      "|  male|  1|null|\n",
      "|female|  2|   3|\n",
      "|  male|  3|   4|\n",
      "+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename a column name\n",
    "ds1=ds.withColumnRenamed('C','CC')\n",
    "ds1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     A|\n",
      "+------+\n",
      "|  male|\n",
      "|female|\n",
      "|  male|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns, ds does not change\n",
    "drop_name = ['B','C']\n",
    "ds.drop(*drop_name).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  8|120.2| 19.6|     11.6| 13.2|\n",
      "|  9|  8.6|  2.1|      1.0|  4.8|\n",
      "| 12|214.7| 24.0|      4.0| 17.4|\n",
      "| 14| 97.5|  7.6|      7.2|  9.7|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "ds = spark.read.csv(path='Advertising.csv',\n",
    "                    header=True,\n",
    "                    inferSchema=True)\n",
    "ds[ds.Newspaper<20].show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+--------------------+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|             tv_norm|\n",
      "+---+-----+-----+---------+-----+--------------------+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|0.007824268493802813|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|0.001513167961643...|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|5.848649200061207E-4|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|0.005151571824472517|\n",
      "+---+-----+-----+---------+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+---+-----+-----+---------+-----+------------------+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|            log_tv|\n",
      "+---+-----+-----+---------+-----+------------------+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|  5.43851399704132|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|3.7954891891721947|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|2.8449093838194073|\n",
      "|  4|151.5| 41.3|     58.5| 18.5| 5.020585624949423|\n",
      "+---+-----+-----+---------+-----+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ceate a new column\n",
    "import pyspark.sql.functions as F\n",
    "ds.groupBy().agg(F.sum(\"TV\")).collect()[0][0]\n",
    "ds.withColumn('tv_norm', ds.TV/ds.groupBy().agg(F.sum(\"TV\")).collect()[0][0]).show(4)\n",
    "ds.withColumn('log_tv',F.log(ds.TV)).show(4)\n",
    "ds.withColumn('tv+10', ds.TV+10).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "| A0| B0| C0| D0|\n",
      "| A1| B1| C1| D1|\n",
      "| A2| B2| C2| D2|\n",
      "| A3| B3| C3| D3|\n",
      "+---+---+---+---+\n",
      "\n",
      "+---+---+---+---+\n",
      "|  A|  F|  G|  H|\n",
      "+---+---+---+---+\n",
      "| A0| B4| C4| D4|\n",
      "| A1| B5| C5| D5|\n",
      "| A6| B6| C6| D6|\n",
      "| A7| B7| C7| D7|\n",
      "+---+---+---+---+\n",
      "\n",
      "+---+---+---+---+----+----+----+\n",
      "|  A|  B|  C|  D|   F|   G|   H|\n",
      "+---+---+---+---+----+----+----+\n",
      "| A0| B0| C0| D0|  B4|  C4|  D4|\n",
      "| A1| B1| C1| D1|  B5|  C5|  D5|\n",
      "| A2| B2| C2| D2|null|null|null|\n",
      "| A3| B3| C3| D3|null|null|null|\n",
      "+---+---+---+---+----+----+----+\n",
      "\n",
      "+---+----+----+----+---+---+---+\n",
      "|  A|   B|   C|   D|  F|  G|  H|\n",
      "+---+----+----+----+---+---+---+\n",
      "| A0|  B0|  C0|  D0| B4| C4| D4|\n",
      "| A1|  B1|  C1|  D1| B5| C5| D5|\n",
      "| A6|null|null|null| B6| C6| D6|\n",
      "| A7|null|null|null| B7| C7| D7|\n",
      "+---+----+----+----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe join\n",
    "leftp = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                    index=[0, 1, 2, 3])\n",
    "\n",
    "rightp = pd.DataFrame({'A': ['A0', 'A1', 'A6', 'A7'],\n",
    "                       'F': ['B4', 'B5', 'B6', 'B7'],\n",
    "                       'G': ['C4', 'C5', 'C6', 'C7'],\n",
    "                       'H': ['D4', 'D5', 'D6', 'D7']},\n",
    "                       index=[4, 5, 6, 7])\n",
    "\n",
    "lefts = spark.createDataFrame(leftp)\n",
    "rights = spark.createDataFrame(rightp)\n",
    "lefts.show()\n",
    "rights.show()\n",
    "\n",
    "#left join\n",
    "leftp.merge(rightp,on='A',how='left')\n",
    "\n",
    "lefts.join(rights,on='A',how='left').orderBy('A',ascending=True).show()\n",
    "\n",
    "#right join\n",
    "leftp.merge(rightp,on='A',how='right')\n",
    "lefts.join(rights,on='A',how='right').orderBy('A',ascending=True).show()\n",
    "\n",
    "#inner join\n",
    "leftp.merge(rightp,on='A',how='inner')\n",
    "lefts.join(rights,on='A',how='inner').orderBy('A',ascending=True).show()\n",
    "\n",
    "#full join\n",
    "leftp.merge(rightp,on='A',how='outer')\n",
    "lefts.join(rights,on='A',how='full').orderBy('A',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  col1  col2  col3 concat\n",
      "0    a     2     3     a2\n",
      "1    b     5     6     b5\n",
      "2    c     8     9     c8\n",
      "3    a     2     3     a2\n",
      "4    b     5     6     b5\n",
      "+----+----+----+------+\n",
      "|col1|col2|col3|concat|\n",
      "+----+----+----+------+\n",
      "|   a|   2|   3|    a2|\n",
      "|   b|   5|   6|    b5|\n",
      "|   c|   8|   9|    c8|\n",
      "|   a|   2|   3|    a2|\n",
      "|   b|   5|   6|    b5|\n",
      "|   c|   8|   9|    c8|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concat columns\n",
    "my_list = [('a', 2, 3),\n",
    "           ('b', 5, 6),\n",
    "           ('c', 8, 9),\n",
    "           ('a', 2, 3),\n",
    "           ('b', 5, 6),\n",
    "           ('c', 8, 9)]\n",
    "col_name = ['col1', 'col2', 'col3']\n",
    "dp = pd.DataFrame(my_list,columns=col_name)\n",
    "ds = spark.createDataFrame(my_list,schema=col_name)\n",
    "dp['concat'] = dp.apply(lambda x:'%s%s'%(x['col1'],x['col2']),axis=1)\n",
    "print(dp.head())\n",
    "ds.withColumn('concat',F.concat('col1','col2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+\n",
      "|col1|min(col2)|avg(col3)|\n",
      "+----+---------+---------+\n",
      "|   c|        8|      9.0|\n",
      "|   b|        5|      6.0|\n",
      "|   a|        2|      3.0|\n",
      "+----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby\n",
    "dp.groupby(['col1']).agg({'col2':'min','col3':'mean'})\n",
    "ds.groupBy(['col1']).agg({'col2': 'min', 'col3': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|   2|   5|   8|\n",
      "+----+----+----+----+\n",
      "|   c|null|null|  18|\n",
      "|   b|null|  12|null|\n",
      "|   a|   6|null|null|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pivot table\n",
    "pd.pivot_table(dp, values='col3', index='col1', columns='col2', aggfunc=np.sum)\n",
    "ds.groupBy(['col1']).pivot('col2').sum('col3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  C  rank\n",
      "0  a  m  1   2.0\n",
      "1  b  m  2   1.0\n",
      "2  c  n  3   2.0\n",
      "3  d  n  6   1.0\n",
      "+---+---+---+----+\n",
      "|  A|  B|  C|rank|\n",
      "+---+---+---+----+\n",
      "|  b|  m|  2|   1|\n",
      "|  a|  m|  1|   2|\n",
      "|  d|  n|  6|   1|\n",
      "|  c|  n|  3|   2|\n",
      "+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window\n",
    "d = {'A':['a','b','c','d'],'B':['m','m','n','n'],'C':[1,2,3,6]}\n",
    "dp = pd.DataFrame(d)\n",
    "ds = spark.createDataFrame(dp)\n",
    "dp['rank'] = dp.groupby('B')['C'].rank('dense',ascending=False)\n",
    "print(dp.head())\n",
    "from pyspark.sql.window import Window\n",
    "w = Window.partitionBy('B').orderBy(ds.C.desc())\n",
    "ds = ds.withColumn('rank',F.rank().over(w))\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Score  Rank_dense  Rank\n",
      "0   1   4.00         1.0   1.0\n",
      "1   2   4.00         1.0   1.0\n",
      "2   3   3.85         2.0   3.0\n",
      "3   4   3.65         3.0   4.0\n",
      "4   5   3.65         3.0   4.0\n",
      "+---+-----+----------------+----------+\n",
      "| Id|Score|Rank_spark_dense|Rank_spark|\n",
      "+---+-----+----------------+----------+\n",
      "|  1|  4.0|               1|         1|\n",
      "|  2|  4.0|               1|         1|\n",
      "|  3| 3.85|               2|         3|\n",
      "|  4| 3.65|               3|         4|\n",
      "|  5| 3.65|               3|         4|\n",
      "|  6|  3.5|               4|         6|\n",
      "+---+-----+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank vs dense_rank\n",
    "d ={'Id':[1,2,3,4,5,6],\n",
    "    'Score': [4.00, 4.00, 3.85, 3.65, 3.65, 3.50]}\n",
    "#\n",
    "data = pd.DataFrame(d)\n",
    "dp = data.copy()\n",
    "ds = spark.createDataFrame(data)\n",
    "dp['Rank_dense'] = dp['Score'].rank(method='dense',ascending =False)\n",
    "dp['Rank'] = dp['Score'].rank(method='min',ascending =False)\n",
    "print(dp.head())\n",
    "#\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "w = Window.orderBy(ds.Score.desc())\n",
    "ds = ds.withColumn('Rank_spark_dense',F.dense_rank().over(w))\n",
    "ds = ds.withColumn('Rank_spark',F.rank().over(w))\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL\")],\n",
    " [\"document\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "vectorizer  = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "model = pipeline.fit(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|vocabList|counts|\n",
      "+---------+------+\n",
      "|   python|   3.0|\n",
      "|    spark|   2.0|\n",
      "|      sql|   1.0|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_counts = model.transform(sentenceData)\\\n",
    "                    .select('rawFeatures').rdd\\\n",
    "                    .map(lambda row: row['rawFeatures'].toArray())\\\n",
    "                    .reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "\n",
    "vocabList = model.stages[1].vocabulary\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
