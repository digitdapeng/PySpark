{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark regression example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_name = ['_c0']\n",
    "df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').\\\n",
    "            load(\"Advertising.csv\",header=True).drop(*drop_name)\n",
    "df.show(5,True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|               200|\n",
      "|   mean|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to dense vector (features and label)\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# I provide two ways to build the features and labels\n",
    "\n",
    "# method 1 (good for small feature):\n",
    "#def transData(row):\n",
    "#    return Row(label=row[\"Sales\"],\n",
    "#               features=Vectors.dense([row[\"TV\"],\n",
    "#                                       row[\"Radio\"],\n",
    "#                                       row[\"Newspaper\"]]))\n",
    "\n",
    "# Method 2 (good for large features):\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define get_dummy\n",
    "def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol,dropLast=False):\n",
    "\n",
    "    '''\n",
    "    Get dummy variables and concat with continuous variables for ml modeling.\n",
    "    :param df: the dataframe\n",
    "    :param categoricalCols: the name list of the categorical data\n",
    "    :param continuousCols:  the name list of the numerical data\n",
    "    :param labelCol:  the name of label column\n",
    "    :param dropLast:  the flag of drop last column\n",
    "    :return: feature matrix\n",
    "\n",
    "    :author: Wenqiang Feng\n",
    "    :email:  von198@gmail.com\n",
    "\n",
    "    >>> df = spark.createDataFrame([\n",
    "                  (0, \"a\",3.5,30),\n",
    "                  (1, \"b\",5.7,50),\n",
    "                  (2, \"c\",20,45.7),\n",
    "                  (3, \"a\", 8.2,25.3),\n",
    "                  (4, \"a\",9.1,32),\n",
    "                  (5, \"c\",2.5,19.4)\n",
    "              ], [\"id\", \"category\",\"cont\",'price'])\n",
    "\n",
    "    >>> indexCol = 'id'\n",
    "    >>> categoricalCols = ['category']\n",
    "    >>> continuousCols = ['cont']\n",
    "    >>> labelCol = []  or labelCol=['price']\n",
    "\n",
    "    >>> mat = get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol)\n",
    "    >>> mat.show()\n",
    "\n",
    "    >>>\n",
    "        +---+-------------+\n",
    "        | id|     features|\n",
    "        +---+-------------+\n",
    "        |  0|[1.0,0.0,0.0]|\n",
    "        |  1|[0.0,0.0,1.0]|\n",
    "        |  2|[0.0,1.0,0.0]|\n",
    "        |  3|[1.0,0.0,0.0]|\n",
    "        |  4|[1.0,0.0,0.0]|\n",
    "        |  5|[0.0,1.0,0.0]|\n",
    "        +---+-------------+\n",
    "    '''\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()),dropLast=dropLast)\n",
    "                 for indexer in indexers ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    if indexCol and labelCol:\n",
    "        # for supervised learning\n",
    "        data = data.withColumn('label',col(labelCol))\n",
    "        return data.select(indexCol,'features','label')\n",
    "    elif not indexCol and labelCol:\n",
    "        # for supervised learning\n",
    "        data = data.withColumn('label',col(labelCol))\n",
    "        return data.select('features','label')\n",
    "    elif indexCol and not labelCol:\n",
    "        # for unsupervised learning\n",
    "        return data.select(indexCol,'features')\n",
    "    elif not indexCol and not labelCol:\n",
    "        # for unsupervised learning\n",
    "        return data.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([StructField(\"id\", IntegerType(), True)\\\n",
    "                       ,StructField(\"cont\", DoubleType(), True)\\\n",
    "                       ,StructField(\"category\", StringType(), True)\\\n",
    "                       ,StructField(\"price\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "                  (0, 3.5, \"a\",30.5),\n",
    "                  (1, 5.0, \"b\", 49.9),\n",
    "                  (2, 8.5, \"c\", 82.4),\n",
    "                  (3, 15.0, \"a\", 13.0),\n",
    "                  (4, 20.4, \"a\", 18.0),\n",
    "                  (5, 15.3, \"c\", 12.3)], \n",
    "                   [\"id\", \"cont\", \"category\",\"price\"],mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-----+\n",
      "| id|cont|category|price|\n",
      "+---+----+--------+-----+\n",
      "|  0| 3.5|       a| 30.5|\n",
      "|  1| 5.0|       b| 49.9|\n",
      "|  2| 8.5|       c| 82.4|\n",
      "|  3|15.0|       a| 13.0|\n",
      "|  4|20.4|       a| 18.0|\n",
      "|  5|15.3|       c| 12.3|\n",
      "+---+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[1.0,0.0,0.0,3.5,...|\n",
      "|  1|[0.0,0.0,1.0,5.0,...|\n",
      "|  2|[0.0,1.0,0.0,8.5,...|\n",
      "|  3|[1.0,0.0,0.0,15.0...|\n",
      "|  4|[1.0,0.0,0.0,20.4...|\n",
      "|  5|[0.0,1.0,0.0,15.3...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example for get_dummy\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "mySchema = StructType([StructField(\"id\", IntegerType(), True)\\\n",
    "                       ,StructField(\"cont\", DoubleType(), True)\\\n",
    "                       ,StructField(\"category\", StringType(), True)\\\n",
    "                       ,StructField(\"price\", DoubleType(), True)])\n",
    "df1 = spark.createDataFrame([\n",
    "                  (0, 3.5, \"a\",30.5),\n",
    "                  (1, 5.0, \"b\", 49.9),\n",
    "                  (2, 8.5, \"c\", 82.4),\n",
    "                  (3, 15.0, \"a\", 13.0),\n",
    "                  (4, 20.4, \"a\", 18.0),\n",
    "                  (5, 15.3, \"c\", 12.3)], \n",
    "                   [\"id\", \"cont\", \"category\",\"price\"],mySchema)\n",
    "indexCol = 'id'\n",
    "categoricalCols = ['category']\n",
    "continuousCols = ['cont',\"price\"]\n",
    "labelCol = []\n",
    "#labelCol=['price']\n",
    "mat = get_dummy(df1,indexCol,categoricalCols,continuousCols,labelCol)\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the dataset to DataFrame\n",
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+-----------------+\n",
      "|   TV|Radio|Newspaper|Sales|         features|\n",
      "+-----+-----+---------+-----+-----------------+\n",
      "|230.1| 37.8|     69.2| 22.1|[230.1,37.8,69.2]|\n",
      "| 44.5| 39.3|     45.1| 10.4| [44.5,39.3,45.1]|\n",
      "| 17.2| 45.9|     69.3|  9.3| [17.2,45.9,69.3]|\n",
      "|151.5| 41.3|     58.5| 18.5|[151.5,41.3,58.5]|\n",
      "|180.8| 10.8|     58.4| 12.9|[180.8,10.8,58.4]|\n",
      "|  8.7| 48.9|     75.0|  7.2|  [8.7,48.9,75.0]|\n",
      "| 57.5| 32.8|     23.5| 11.8| [57.5,32.8,23.5]|\n",
      "|120.2| 19.6|     11.6| 13.2|[120.2,19.6,11.6]|\n",
      "|  8.6|  2.1|      1.0|  4.8|    [8.6,2.1,1.0]|\n",
      "|199.8|  2.6|     21.2| 10.6| [199.8,2.6,21.2]|\n",
      "| 66.1|  5.8|     24.2|  8.6|  [66.1,5.8,24.2]|\n",
      "|214.7| 24.0|      4.0| 17.4| [214.7,24.0,4.0]|\n",
      "| 23.8| 35.1|     65.9|  9.2| [23.8,35.1,65.9]|\n",
      "| 97.5|  7.6|      7.2|  9.7|   [97.5,7.6,7.2]|\n",
      "|204.1| 32.9|     46.0| 19.0|[204.1,32.9,46.0]|\n",
      "|195.4| 47.7|     52.9| 22.4|[195.4,47.7,52.9]|\n",
      "| 67.8| 36.6|    114.0| 12.5|[67.8,36.6,114.0]|\n",
      "|281.4| 39.6|     55.8| 24.4|[281.4,39.6,55.8]|\n",
      "| 69.2| 20.5|     18.3| 11.3| [69.2,20.5,18.3]|\n",
      "|147.3| 23.9|     19.1| 14.6|[147.3,23.9,19.1]|\n",
      "+-----+-----+---------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"TV\", \"Radio\", \"Newspaper\"],outputCol=\"features\")\n",
    "# Now let us use the transform method to transform our dataset\n",
    "assembler.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "|  8.7| 48.9|     75.0|  7.2|\n",
      "| 57.5| 32.8|     23.5| 11.8|\n",
      "|120.2| 19.6|     11.6| 13.2|\n",
      "|  8.6|  2.1|      1.0|  4.8|\n",
      "|199.8|  2.6|     21.2| 10.6|\n",
      "| 66.1|  5.8|     24.2|  8.6|\n",
      "|214.7| 24.0|      4.0| 17.4|\n",
      "| 23.8| 35.1|     65.9|  9.2|\n",
      "| 97.5|  7.6|      7.2|  9.7|\n",
      "|204.1| 32.9|     46.0| 19.0|\n",
      "|195.4| 47.7|     52.9| 22.4|\n",
      "| 67.8| 36.6|    114.0| 12.5|\n",
      "|281.4| 39.6|     55.8| 24.4|\n",
      "| 69.2| 20.5|     18.3| 11.3|\n",
      "|147.3| 23.9|     19.1| 14.6|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+\n",
      "|cont|category|price|\n",
      "+----+--------+-----+\n",
      "| 3.5|       a| 30.5|\n",
      "| 5.0|       b| 49.9|\n",
      "| 8.5|       c| 82.4|\n",
      "|15.0|       a| 13.0|\n",
      "|20.4|       a| 18.0|\n",
      "|15.3|       c| 12.3|\n",
      "+----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df1.drop('id')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+\n",
      "|         features|label|  indexedFeatures|\n",
      "+-----------------+-----+-----------------+\n",
      "|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|\n",
      "| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|\n",
      "| [17.2,45.9,69.3]|  9.3| [17.2,45.9,69.3]|\n",
      "|[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]|\n",
      "|[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|\n",
      "+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deal With Categorical Variables\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0.0: 0, -1.0: 1}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df3 = spark.createDataFrame([(Vectors.dense([-1.0, 0.0]),),\n",
    "#                             (Vectors.dense([0.0, 1.0]),), \n",
    "#                             (Vectors.dense([0.0, 2.0]),)], [\"a\"])\n",
    "indexer = VectorIndexer(maxCategories=2, inputCol=\"a\", outputCol=\"indexed\")\n",
    "model = indexer.fit(df3)\n",
    "model.transform(df3).head().indexed\n",
    "model.categoryMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0.0: 0, -1.0: 1}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "| [4.1,11.6,5.7]|  3.2|\n",
      "| [5.4,29.9,9.4]|  5.3|\n",
      "|[7.3,28.1,41.4]|  5.5|\n",
      "|[7.8,38.9,50.6]|  6.6|\n",
      "| [8.4,27.2,2.1]|  5.7|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "|  [0.7,39.6,8.7]|  1.6|\n",
      "|   [8.6,2.1,1.0]|  4.8|\n",
      "|[11.7,36.9,45.2]|  7.3|\n",
      "|[13.2,15.9,49.6]|  5.6|\n",
      "|[18.7,12.1,23.4]|  6.7|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "(trainingData, testData) = transformed.randomSplit([0.7, 0.3])\n",
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexer and lr in a Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the last rows are the information for Intercept\n",
      "## -------------------------------------------------\n",
      "##   Estimate   |   Std.Error | t Values  |  P-value\n",
      "##   0.046021   0.001679   27.414   0.000000\n",
      "##   0.180169   0.010896   16.535   0.000000\n",
      "##   0.007268   0.007665    0.948   0.344691\n",
      "##   2.835365   0.370348    7.656   0.000000\n",
      "## ---\n",
      "## Mean squared error:  2.991112 , RMSE:  1.729483\n",
      "## Multiple R-squared: 0.896211 ,             Total iterations: 1\n"
     ]
    }
   ],
   "source": [
    "#summary of the model\n",
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    print (\"Note: the last rows are the information for Intercept\")\n",
    "    print (\"##\",\"-------------------------------------------------\")\n",
    "    print (\"##\",\"  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "    coef = np.append(list(model.coefficients),model.intercept)\n",
    "    Summary=model.summary\n",
    "\n",
    "    for i in range(len(Summary.pValues)):\n",
    "        print (\"##\",'{:10.6f}'.format(coef[i]),\\\n",
    "        '{:10.6f}'.format(Summary.coefficientStandardErrors[i]),\\\n",
    "        '{:8.3f}'.format(Summary.tValues[i]),\\\n",
    "        '{:10.6f}'.format(Summary.pValues[i]))\n",
    "\n",
    "    print (\"##\",'---')\n",
    "    print (\"##\",\"Mean squared error: % .6f\" \\\n",
    "           % Summary.meanSquaredError, \", RMSE: % .6f\" \\\n",
    "           % Summary.rootMeanSquaredError )\n",
    "    print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", \\\n",
    "            Total iterations: %i\"% Summary.totalIterations)\n",
    "    \n",
    "modelsummary(model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+------------------+\n",
      "|        features|label|        prediction|\n",
      "+----------------+-----+------------------+\n",
      "| [7.8,38.9,50.6]|  6.6| 10.57065065502108|\n",
      "|[11.7,36.9,45.2]|  7.3|10.350549256693258|\n",
      "|[16.9,43.7,89.4]|  8.7|12.136236800697118|\n",
      "| [17.2,4.1,31.6]|  5.9| 4.595276736599125|\n",
      "|[18.7,12.1,23.4]|  6.7| 6.046067314265413|\n",
      "+----------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1.54934\n",
      "r2_score: 0.8946265735228226\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# calculate R2\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|               200|\n",
      "|   mean|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n",
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-----+-----------------+\n",
      "|         features|label|  indexedFeatures|\n",
      "+-----------------+-----+-----------------+\n",
      "|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|\n",
      "| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|\n",
      "| [17.2,45.9,69.3]|  9.3| [17.2,45.9,69.3]|\n",
      "|[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]|\n",
      "|[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|\n",
      "+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with same Advertising data\n",
    "drop_name = ['_c0']\n",
    "df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').\\\n",
    "            load(\"Advertising.csv\",header=True).drop(*drop_name)\n",
    "df.show(5,True)\n",
    "df.printSchema()\n",
    "df.describe().show()\n",
    "\n",
    "transformed= transData(df)\n",
    "transformed.show(5)\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "| [0.7,39.6,8.7]|  1.6|\n",
      "| [4.1,11.6,5.7]|  3.2|\n",
      "|[7.8,38.9,50.6]|  6.6|\n",
      "| [8.4,27.2,2.1]|  5.7|\n",
      "|  [8.6,2.1,1.0]|  4.8|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "|  [5.4,29.9,9.4]|  5.3|\n",
      "| [7.3,28.1,41.4]|  5.5|\n",
      "|[16.9,43.7,89.4]|  8.7|\n",
      "|[18.8,21.7,50.4]|  7.0|\n",
      "|[25.1,25.7,43.3]|  8.5|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use same train and test data\n",
    "(trainingData, testData) = transformed.randomSplit([0.7, 0.3])\n",
    "#(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.7, 39.6, 8.7]), label=1.6, indexedFeatures=DenseVector([0.7, 39.6, 8.7])),\n",
       " Row(features=DenseVector([5.4, 29.9, 9.4]), label=5.3, indexedFeatures=DenseVector([5.4, 29.9, 9.4])),\n",
       " Row(features=DenseVector([7.8, 38.9, 50.6]), label=6.6, indexedFeatures=DenseVector([7.8, 38.9, 50.6])),\n",
       " Row(features=DenseVector([8.4, 27.2, 2.1]), label=5.7, indexedFeatures=DenseVector([8.4, 27.2, 2.1])),\n",
       " Row(features=DenseVector([8.6, 2.1, 1.0]), label=4.8, indexedFeatures=DenseVector([8.6, 2.1, 1.0]))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.collect()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\",\\\n",
    "                                  maxIter=10, regParam=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, glr])\n",
    "\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the last rows are the information for Intercept\n",
      "## -------------------------------------------------\n",
      "##   Estimate   |   Std.Error | t Values  |  P-value\n",
      "##   0.044130   0.001685   26.183   0.000000\n",
      "##   0.176380   0.010259   17.193   0.000000\n",
      "##   0.000266   0.006635    0.040   0.968110\n",
      "##   3.486282   0.375172    9.292   0.000000\n",
      "## ---\n"
     ]
    }
   ],
   "source": [
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    print (\"Note: the last rows are the information for Intercept\")\n",
    "    print (\"##\",\"-------------------------------------------------\")\n",
    "    print (\"##\",\"  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "    coef = np.append(list(model.coefficients),model.intercept)\n",
    "    Summary=model.summary\n",
    "\n",
    "    for i in range(len(Summary.pValues)):\n",
    "        print (\"##\",'{:10.6f}'.format(coef[i]),\\\n",
    "        '{:10.6f}'.format(Summary.coefficientStandardErrors[i]),\\\n",
    "        '{:8.3f}'.format(Summary.tValues[i]),\\\n",
    "        '{:10.6f}'.format(Summary.pValues[i]))\n",
    "\n",
    "    print (\"##\",'---')\n",
    "#     print (\"##\",\"Mean squared error: % .6f\" \\\n",
    "#            % Summary.meanSquaredError, \", RMSE: % .6f\" \\\n",
    "#            % Summary.rootMeanSquaredError )\n",
    "#     print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", \\\n",
    "#             Total iterations: %i\"% Summary.totalIterations)\n",
    "\n",
    "\n",
    "modelsummary(model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+------------------+\n",
      "|        features|label|        prediction|\n",
      "+----------------+-----+------------------+\n",
      "|  [4.1,11.6,5.7]|  3.2| 5.714736129608138|\n",
      "|   [8.6,2.1,1.0]|  4.8|  4.23646503576621|\n",
      "| [8.7,48.9,75.0]|  7.2|12.515118025391684|\n",
      "|[11.7,36.9,45.2]|  7.3| 10.52303137780692|\n",
      "|[18.8,21.7,50.4]|  7.0|  8.15676524189303|\n",
      "+----------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1.67846\n",
      "r2_score: 0.8911232952954501\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "#R2\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example with power plant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.epigno.systems/2018/02/18/machine-learning-with-pyspark-linear-regression/\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+\n",
      "|   AT|    V|     AP|   RH|    PE|\n",
      "+-----+-----+-------+-----+------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|\n",
      "|25.18|62.96|1020.04|59.08|444.37|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|\n",
      "|20.86|57.32|1010.24|76.64|446.48|\n",
      "|10.82| 37.5|1009.23|96.62| 473.9|\n",
      "+-----+-----+-------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = \"pp_data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "data = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|                AT|                 V|                AP|               RH|                PE|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|             47840|             47840|             47840|            47840|             47840|\n",
      "|   mean|19.651231187290833| 54.30580372073524|1013.2590781772577| 73.3089778428094|454.36500940635983|\n",
      "| stddev|7.4521616583400085|12.707361709685806| 5.938535418520848|14.59965835208147|17.066281466837733|\n",
      "|    min|              1.81|             25.36|            992.89|            25.56|            420.26|\n",
      "|    max|             37.11|             81.56|            1033.3|           100.16|            495.76|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- AT: double (nullable = true)\n",
      " |-- V: double (nullable = true)\n",
      " |-- AP: double (nullable = true)\n",
      " |-- RH: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#features = [\"temperature\", \"exhaust_vacuum\", \"ambient_pressure\", \"relative_humidity\"]\n",
    "from pyspark.sql.functions import col, abs\n",
    "features = [\"AT\", \"V\", \"AP\", \"RH\"]\n",
    "lr_data = data.select(col(\"PE\").alias(\"label\"), *features)\n",
    "lr_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+-----+\n",
      "| label|   AT|    V|     AP|   RH|\n",
      "+------+-----+-----+-------+-----+\n",
      "|463.26|14.96|41.76|1024.07|73.17|\n",
      "|444.37|25.18|62.96|1020.04|59.08|\n",
      "|488.56| 5.11| 39.4|1012.16|92.14|\n",
      "|446.48|20.86|57.32|1010.24|76.64|\n",
      "| 473.9|10.82| 37.5|1009.23|96.62|\n",
      "|443.67|26.27|59.44|1012.23|58.77|\n",
      "|467.35|15.89|43.96|1014.02|75.24|\n",
      "|478.42| 9.48|44.71|1019.12|66.43|\n",
      "|475.98|14.64| 45.0|1021.78|41.25|\n",
      "| 477.5|11.74|43.56|1015.14|70.72|\n",
      "|453.02|17.99|43.72|1008.64|75.04|\n",
      "|453.99|20.14|46.93|1014.66|64.22|\n",
      "|440.29|24.34| 73.5|1011.31|84.15|\n",
      "|451.28|25.71|58.59|1012.77|61.83|\n",
      "|433.99|26.19|69.34|1009.48|87.59|\n",
      "|462.19|21.42|43.79|1015.76|43.08|\n",
      "|467.54|18.21| 45.0|1022.86|48.84|\n",
      "| 477.2|11.04|41.74| 1022.6|77.51|\n",
      "|459.85|14.45|52.75|1023.97|63.59|\n",
      "| 464.3|13.97|38.47|1015.15|55.28|\n",
      "+------+-----+-----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = lr_data.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This step is the beautiful part about Spark. You can build your ML pipeline and Spark will do all the heavy lifting for you.\n",
    "This is how things work in our case:\n",
    "we put all features into a vector\n",
    "since we are dealing with numerical data, we scale those features\n",
    "we chose the algorithm (in our case is linear regression)\n",
    "and we create the pipeline with the steps and in the order mentioned above\n",
    "\"\"\"\n",
    "vectorizer = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "lr = LinearRegression(maxIter=10, regParam=.01)\n",
    "print(lr.explainParams())\n",
    "stages = [vectorizer, standardScaler, lr]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+-----+--------------------+--------------------+------------------+\n",
      "| label|   AT|    V|     AP|   RH|   unscaled_features|            features|        prediction|\n",
      "+------+-----+-----+-------+-----+--------------------+--------------------+------------------+\n",
      "|420.26|24.27|63.87|1018.88|53.96|[24.27,63.87,1018...|[3.25621100274884...| 446.3474964605456|\n",
      "|420.26|24.27|63.87|1018.88|53.96|[24.27,63.87,1018...|[3.25621100274884...| 446.3474964605456|\n",
      "|425.11|32.56|68.14|1004.02|35.04|[32.56,68.14,1004...|[4.36844788831901...|  431.045327024459|\n",
      "|425.12|31.74|72.58|1007.26|59.58|[31.74,72.58,1007...|[4.25843169457142...|427.97721413740186|\n",
      "|425.12|31.74|72.58|1007.26|59.58|[31.74,72.58,1007...|[4.25843169457142...|427.97721413740186|\n",
      "|425.14|29.67|71.98|1005.16|67.75|[29.67,71.98,1005...|[3.98070788840372...|430.79761540492075|\n",
      "|425.16|30.71|71.85|1008.07|72.05|[30.71,71.85,1008...|[4.12024062193725...|428.28306586751086|\n",
      "|425.16|30.71|71.85|1008.07|72.05|[30.71,71.85,1008...|[4.12024062193725...|428.28306586751086|\n",
      "|425.17|32.66|73.68|1014.64|40.88|[32.66,73.68,1014...|[4.38186449731262...| 429.2698947403026|\n",
      "|425.17|32.66|73.68|1014.64|40.88|[32.66,73.68,1014...|[4.38186449731262...| 429.2698947403026|\n",
      "|425.17|32.66|73.68|1014.64|40.88|[32.66,73.68,1014...|[4.38186449731262...| 429.2698947403026|\n",
      "|425.18|32.84|68.14|1003.59|43.88|[32.84,68.14,1003...|[4.40601439350112...|429.08717844745024|\n",
      "|425.18|32.84|68.14|1003.59|43.88|[32.84,68.14,1003...|[4.40601439350112...|429.08717844745024|\n",
      "|425.19|31.92|69.13|1000.77|58.91|[31.92,69.13,1000...|[4.28258159075992...|428.14951897218265|\n",
      "|425.19|31.92|69.13|1000.77|58.91|[31.92,69.13,1000...|[4.28258159075992...|428.14951897218265|\n",
      "| 425.2| 29.2|70.79| 1004.8|63.74|[29.2,70.79,1004....|[3.91764982613376...|432.60995415246856|\n",
      "|425.21| 30.2|71.85|1008.25| 59.1|[30.2,71.85,1008....|[4.05181591606984...| 431.3211433541805|\n",
      "|425.21|31.12|67.69| 1005.3|50.46|[31.12,67.69,1005...|[4.17524871881105...|431.66127255284164|\n",
      "|425.21|32.19|69.13|1000.45|48.22|[32.19,69.13,1000...|[4.31880643504266...|429.26661148074567|\n",
      "|425.21|32.19|69.13|1000.45|48.22|[32.19,69.13,1000...|[4.31880643504266...|429.26661148074567|\n",
      "+------+-----+-----+-------+-----+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(training)\n",
    "pred = model.transform(test)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455.9505007439303\n",
      "[-14.69528846198947,-3.0050825088904434,0.35993671509112307,-2.2748206161129683]\n"
     ]
    }
   ],
   "source": [
    "# The intercept is as follows:\n",
    "intercept = model.stages[2].intercept\n",
    "print(intercept)\n",
    "# The coefficents (i.e., weights) are as follows:\n",
    "weights = model.stages[2].coefficients\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AT', 'V', 'AP', 'RH']\n",
      "<zip object at 0x0000022A78FE87C8>\n",
      "Linear Regression Equation: y = 455.9505007439303 -14.69528846198947 * AT -3.0050825088904434 * V 0.35993671509112307 * AP -2.2748206161129683 * RH\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the column names (without PE)\n",
    "featuresNoLabel = [col for col in data.columns if col != \"PE\"]\n",
    "print(featuresNoLabel)\n",
    "# Merge the weights and labels\n",
    "coefficents = zip(weights, featuresNoLabel)\n",
    "print(coefficents)\n",
    "# Now let's sort the coefficients from greatest absolute weight most to the least absolute weight\n",
    "#coefficents.sort(key=lambda tup: abs(tup[0]), reverse=True)\n",
    "\n",
    "equation = \"y = {intercept}\".format(intercept=intercept)\n",
    "variables = []\n",
    "for x in coefficents:\n",
    "    #print(x[0],x[1])\n",
    "    #weight = abs(x[0])\n",
    "    weight = x[0]\n",
    "    name = x[1]\n",
    "    #symbol = \"+\" if (x[0] > 0) else \"-\"\n",
    "    equation += (\" {} * {}\".format(weight, name))\n",
    "\n",
    "# Finally here is our equation\n",
    "print(\"Linear Regression Equation: \" + equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.524\n",
      "MSE: 20.465\n",
      "MAE: 3.612\n",
      "r2: 0.929\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = eval.evaluate(pred)\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = eval.evaluate(pred, {eval.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = eval.evaluate(pred, {eval.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = eval.evaluate(pred, {eval.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another liear regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/kharpann/perform-linear-regression-on-big-data-using-python-spark-and-mllib-b1204769547e\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "spark = SparkSession.builder.appName('lrex').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\").load(\"./LinearRegression/sample_linear_regression_data.txt\")\n",
    "# Split the data\n",
    "split_data = data.randomSplit([0.8,0.2])\n",
    "train_data,test_data = split_data\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0073350710225801715,0.8313757584337543,-0.8095307954684084,2.441191686884721,0.5191713795290003,1.1534591903547016,-0.2989124112808717,-0.5128514186201779,-0.619712827067017,0.6956151804322931]\n",
      "0.14228558260358093\n"
     ]
    }
   ],
   "source": [
    "lin_reg=LinearRegression(featuresCol='features',labelCol='label',predictionCol='prediction') \n",
    "lin_reg_model=lin_reg.fit(training_data)\n",
    "print(lin_reg_model.coefficients)\n",
    "print(lin_reg_model.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.731146318219317\n",
      "0.052911772068891194\n"
     ]
    }
   ],
   "source": [
    "result = lin_reg_model.evaluate(test_data)\n",
    "print(result.rootMeanSquaredError)\n",
    "print(result.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|          prediction|\n",
      "+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|  0.8366449600482407|\n",
      "|(10,[0,1,2,3,4,5,...|  -1.475284763550391|\n",
      "|(10,[0,1,2,3,4,5,...|  -2.680136401522974|\n",
      "|(10,[0,1,2,3,4,5,...| -0.6346861633647207|\n",
      "|(10,[0,1,2,3,4,5,...|  -0.976510689078842|\n",
      "|(10,[0,1,2,3,4,5,...| -1.3820341784499024|\n",
      "|(10,[0,1,2,3,4,5,...|  1.6690235491472345|\n",
      "|(10,[0,1,2,3,4,5,...|  0.9290491838587673|\n",
      "|(10,[0,1,2,3,4,5,...|  0.5789191740943999|\n",
      "|(10,[0,1,2,3,4,5,...| -1.5735323090303452|\n",
      "|(10,[0,1,2,3,4,5,...|  -2.415103670384772|\n",
      "|(10,[0,1,2,3,4,5,...| -0.7680465872064948|\n",
      "|(10,[0,1,2,3,4,5,...| -3.0756131143558623|\n",
      "|(10,[0,1,2,3,4,5,...| 0.16437788237675283|\n",
      "|(10,[0,1,2,3,4,5,...| -2.7189163477793263|\n",
      "|(10,[0,1,2,3,4,5,...|  -4.603500793204407|\n",
      "|(10,[0,1,2,3,4,5,...|-0.11085316828698039|\n",
      "|(10,[0,1,2,3,4,5,...| 0.47959665597587847|\n",
      "|(10,[0,1,2,3,4,5,...|  1.2616592891872596|\n",
      "|(10,[0,1,2,3,4,5,...| -0.2572880603356725|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lin_reg_model.transform(test_data.select('features'))\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization\n",
    "The elastic net penalty is a mixture of these two; alpha=0.5 tends to select the groups as in or out. \n",
    "\n",
    "If α =1, the elastic net performs much like the LASSO method. \n",
    "If α =0, the elastic net performs much like the Ridge method\n",
    "\n",
    "LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", maxIter=100,\n",
    "regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, standardization=True, solver=\"auto\",\n",
    "weightCol=None, aggregationDepth=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
