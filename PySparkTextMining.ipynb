{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytesseract  # an Optical Character Recognition(OCR) tool for Python\n",
    "# import tesseract  #optical character recognition engine with open-source code, popular OCR-library\n",
    "# import PythonMagick  # create, edit, and compose images\n",
    "# import PyPDF2  #use for adding, splitting, merging, cropping, and transforming pages in your PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Text Mining\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2txt(img_dir):\n",
    "    \"\"\"\n",
    "    convert images to text\n",
    "    \"\"\"\n",
    "    import os, PythonMagick\n",
    "    from datetime import datetime\n",
    "    import PyPDF2\n",
    "\n",
    "    from PIL import Image\n",
    "    import pytesseract\n",
    "    \n",
    "    pytesseract.pytesseract.tesseract_cmd=r'C:\\Users\\dwang\\AppData\\Local\\Tesseract-OCR'\n",
    "\n",
    "    #f = open('doc4img.txt','wa')\n",
    "    for img in [img_file for img_file in os.listdir(img_dir)\n",
    "                if (img_file.endswith(\".png\") or\n",
    "                    img_file.endswith(\".jpg\") or\n",
    "                    img_file.endswith(\".jpeg\"))]:\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        input_img = img_dir + \"/\" + img\n",
    "\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(img)\n",
    "        print('Converting ' + img +'.......')\n",
    "        print('--------------------------------------------------------------------')\n",
    "\n",
    "        # extract the text information from images\n",
    "        text = pytesseract.image_to_string(Image.open(input_img))\n",
    "        print(text)\n",
    "\n",
    "        # ouput text file\n",
    "        #f.write( img + \"\\n\")\n",
    "        #f.write(text.encode('utf-8'))\n",
    "\n",
    "\n",
    "        print(\"CPU Time for converting\" + img +\":\"+ str(datetime.now() - start_time) +\"\\n\")\n",
    "        #f.write( \"\\n-------------------------------------------------------------\\n\")\n",
    "\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"img\"\n",
    "img2txt(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2txt(pdf_dir,image_dir):\n",
    "    \"\"\"\n",
    "    convert PDF to text\n",
    "    \"\"\"\n",
    "\n",
    "    import os, PythonMagick\n",
    "    from datetime import datetime\n",
    "    import PyPDF2\n",
    "\n",
    "    from PIL import Image\n",
    "    import pytesseract\n",
    "\n",
    "    f = open('doc.txt','wa')\n",
    "    for pdf in [pdf_file for pdf_file in os.listdir(pdf_dir) if pdf_file.endswith(\".pdf\")]:\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        input_pdf = pdf_dir + \"/\" + pdf\n",
    "\n",
    "        pdf_im = PyPDF2.PdfFileReader(file(input_pdf, \"rb\"))\n",
    "        npage = pdf_im.getNumPages()\n",
    "\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(pdf)\n",
    "        print('Converting %d pages.' % npage)\n",
    "        print('--------------------------------------------------------------------')\n",
    "\n",
    "        f.write( \"\\n--------------------------------------------------------------------\\n\")\n",
    "\n",
    "        for p in range(npage):\n",
    "\n",
    "            pdf_file = input_pdf + '[' + str(p) +']'\n",
    "            image_file =  image_dir  + \"/\" + pdf+ '_' + str(p)+ '.png'\n",
    "\n",
    "            # convert PDF files to Images\n",
    "            im = PythonMagick.Image()\n",
    "            im.density('300')\n",
    "            im.read(pdf_file)\n",
    "            im.write(image_file)\n",
    "\n",
    "            # extract the text information from images\n",
    "            text = pytesseract.image_to_string(Image.open(image_file))\n",
    "\n",
    "            #print(text)\n",
    "\n",
    "            # ouput text file\n",
    "            f.write( pdf + \"\\n\")\n",
    "            f.write(text.encode('utf-8'))\n",
    "\n",
    "\n",
    "        print \"CPU Time for converting\" + pdf +\":\"+ str(datetime.now() - start_time) +\"\\n\"\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = r\"pdf\"\n",
    "image_dir = r\"Image\"\n",
    "pdf2txt(pdf_dir,image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('doc.txt','w')\n",
    "text=\"abc\"\n",
    "f.write(text)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2txt(audio_dir):\n",
    "    ''' convert audio to text'''\n",
    "\n",
    "    import speech_recognition as sr\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    f = open('doc.txt','w')\n",
    "    for audio_n in [audio_file for audio_file in os.listdir(audio_dir) \\\n",
    "                  if audio_file.endswith(\".wav\")]:\n",
    "\n",
    "        filename = audio_dir + \"/\" + audio_n\n",
    "\n",
    "        # Read audio data\n",
    "        with sr.AudioFile(filename) as source:\n",
    "            audio = r.record(source)  # read the entire audio file\n",
    "\n",
    "        # Google Speech Recognition\n",
    "        text = r.recognize_google(audio)\n",
    "\n",
    "        # ouput text file\n",
    "        f.write( audio_n + \": \")\n",
    "        f.write(text)  #.encode('utf-8'))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        print('You said: ' + text)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recording something\n",
    "import sys, getopt\n",
    "import speech_recognition as sr\n",
    "\n",
    "audio_filename = sys.argv[1]\n",
    "\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    r.adjust_for_ambient_noise(source)\n",
    "    print(\"Hey there, say something, I am recording!\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Done listening!\")\n",
    "\n",
    "with open(audio_filename, \"wb\") as f:\n",
    "    f.write(audio.get_wav_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if a row only contains whitespace\n",
    "def check_blanks(data_str):\n",
    "    is_blank = str(data_str.isspace())\n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether the language of the text content is english or not\n",
    "def check_lang(data_str):\n",
    "    predict_lang = langid.classify(data_str)\n",
    "    if predict_lang[1] >= .9:\n",
    "        language = predict_lang[0]\n",
    "    else:\n",
    "        language = 'NA'\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features\n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    return cleaned_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes stop words\n",
    "def remove_stops(data_str):\n",
    "    # expects a string\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stops:\n",
    "            # rebuild cleaned_str\n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging text\n",
    "def tag_and_remove(data_str):\n",
    "    cleaned_str = ' '\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "\n",
    "    # break string into 'words'\n",
    "    text = data_str.split()\n",
    "\n",
    "    # tag the text and keep only those with the right tags\n",
    "    tagged_text = pos_tag(text)\n",
    "    for tagged_word in tagged_text:\n",
    "        if tagged_word[1] in nltk_tags:\n",
    "            cleaned_str += tagged_word[0] + ' '\n",
    "\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "def lemmatize(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = data_str.split()\n",
    "    tagged_words = pos_tag(text)\n",
    "    for word in tagged_words:\n",
    "        if 'v' in word[1].lower():\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
    "        else:\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
    "        if list_pos == 0:\n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "            cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessing function in PySpark\n",
    "# setup pyspark udf function\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "#import preproc as pp\n",
    "\n",
    "\"\"\"\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())\n",
    "\"\"\"\n",
    "check_lang_udf = udf(check_lang, StringType())\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(lemmatize, StringType())\n",
    "check_blanks_udf = udf(check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "data_rdd = sc.textFile(\"data/raw_data.txt\")\n",
    "parts_rdd = data_rdd.map(lambda l: l.split(\" \"))\n",
    "# Filter bad rows out\n",
    "garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "typed_rdd = garantee_col_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
    "\n",
    "typed_rdd.collect()\n",
    "\n",
    "#Create DataFrame\n",
    "data_df = sqlContext.createDataFrame(typed_rdd, [\"text\", \"id\", \"label\"])\n",
    "\n",
    "# get the raw columns\n",
    "raw_cols = data_df.columns\n",
    "\n",
    "#data_df.show()\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language identification\n",
    "lang_df = data_df.withColumn(\"lang\", check_lang_udf(data_df[\"text\"]))\n",
    "en_df = lang_df.filter(lang_df[\"lang\"] == \"en\")\n",
    "#en_df.show(4)\n",
    "lang_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "rm_stops_df = en_df.select(raw_cols).withColumn(\"stop_text\", remove_stops_udf(en_df[\"text\"]))\n",
    "rm_stops_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant features\n",
    "rm_features_df = rm_stops_df.select(raw_cols+[\"stop_text\"])\\\n",
    "                            .withColumn(\"feat_text\", \\\n",
    "                            remove_features_udf(rm_stops_df[\"stop_text\"]))\n",
    "rm_features_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tg the words\n",
    "tagged_df = rm_features_df.select(raw_cols+[\"feat_text\"]) \\\n",
    "                          .withColumn(\"tagged_text\", \\\n",
    "                           tag_and_remove_udf(rm_features_df.feat_text))\n",
    "tagged_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization of words\n",
    "lemm_df = tagged_df.select(raw_cols+[\"tagged_text\"]) \\\n",
    "                   .withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"]))\n",
    "lemm_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blank and drop duplication\n",
    "check_blanks_df = lemm_df.select(raw_cols+[\"lemm_text\"])\\\n",
    "                             .withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "# remove blanks\n",
    "no_blanks_df = check_blanks_df.filter(check_blanks_df[\"is_blank\"] == \"False\")\n",
    "\n",
    "# drop duplicates\n",
    "dedup_df = no_blanks_df.dropDuplicates(['text', 'label'])\n",
    "dedup_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add unique id\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "# Create Unique ID\n",
    "dedup_df = dedup_df.withColumn(\"uid\", monotonically_increasing_id())\n",
    "dedup_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final dataset\n",
    "data = dedup_df.select('uid','id', 'text','label')\n",
    "data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test set\n",
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayse pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "# vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Naive Bayes model\n",
    "nb = NaiveBayes()\n",
    "\n",
    "# Pipeline Architecture\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"text\", \"label\", \"prediction\").show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Sentiment Analysis example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').\\\n",
    "            load(\"data/newtwitter.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-ascii\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re\n",
    "\n",
    "# remove non ASCII characters\n",
    "def strip_non_ascii(data_str):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "# setup pyspark udf function\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
    "\n",
    "df = df.withColumn('text_non_asci',strip_non_ascii_udf(df['text']))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed abbreviation\n",
    "def fix_abbreviation(data_str):\n",
    "    data_str = data_str.lower()\n",
    "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
    "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
    "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
    "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
    "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
    "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
    "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
    "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
    "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
    "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
    "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
    "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
    "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
    "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
    "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
    "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
    "    data_str = re.sub(r'rt\\b', '', data_str)\n",
    "    data_str = data_str.strip()\n",
    "    return data_str\n",
    "\n",
    "fix_abbreviation_udf = udf(fix_abbreviation, StringType())\n",
    "df = df.withColumn('fixed_abbrev',fix_abbreviation_udf(df['text_non_asci']))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant features\n",
    "df = df.withColumn('removed',remove_features_udf(df['fixed_abbrev']))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment main function\n",
    "from pyspark.sql.types import FloatType\n",
    "from textblob import TextBlob\n",
    "def sentiment_analysis(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())\n",
    "df  = df.withColumn(\"sentiment_score\", sentiment_analysis_udf( df['removed'] ))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Sentiment Classification\n",
    "def condition(r):\n",
    "    if (r >=0.1):\n",
    "        label = \"positive\"\n",
    "    elif(r <= -0.1):\n",
    "        label = \"negative\"\n",
    "    else:\n",
    "        label = \"neutral\"\n",
    "    return label\n",
    "\n",
    "sentiment_udf = udf(lambda x: condition(x), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model: Latent Dirichlet Allocation\n",
    "a topic model is a unsupervised model for discovering the abstract “topics” that occur in a collection of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = spark.read.load(\"../data/airlines.csv\", format=\"csv\", header=True)\n",
    "rawdata.show(5)\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, DoubleType, DateType\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import langid\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#correct the data schema\n",
    "rawdata = rawdata.withColumn('rating', rawdata.rating.cast('float'))\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\n",
    "# 21-Jun-14 <----> %d-%b-%y\n",
    "to_date =  udf (lambda x: datetime.strptime(x, '%d-%b-%y'), DateType())\n",
    "\n",
    "rawdata = rawdata.withColumn('date', to_date(col('date')))\n",
    "rawdata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = rawdata.withColumn('non_asci', strip_non_ascii_udf(rawdata['review']))\n",
    "rawdata = rawdata.select(raw_cols+['non_asci'])\\\n",
    "                 .withColumn('fixed_abbrev',fix_abbreviation_udf(rawdata['non_asci']))\n",
    "rawdata = rawdata.select(raw_cols+['fixed_abbrev'])\\\n",
    "                  .withColumn('stop_text',remove_stops_udf(rawdata['fixed_abbrev']))\n",
    "rawdata = rawdata.select(raw_cols+['stop_text'])\\\n",
    "                 .withColumn('feat_text',remove_features_udf(rawdata['stop_text']))\n",
    "\n",
    "rawdata = rawdata.select(raw_cols+['feat_text'])\\\n",
    "                  .withColumn('tagged_text',tag_and_remove_udf(rawdata['feat_text']))\n",
    "\n",
    "rawdata = rawdata.select(raw_cols+['tagged_text']) \\\n",
    "                  .withColumn('lemm_text',lemmatize_udf(rawdata['tagged_text'])\n",
    "\n",
    "                              rawdata = rawdata.select(raw_cols+['lemm_text']) \\\n",
    "                  .withColumn(\"is_blank\", check_blanks_udf(rawdata[\"lemm_text\"]))\n",
    "rawdata = rawdata.withColumn(\"uid\", monotonically_increasing_id())\n",
    "data = rawdata.filter(rawdata[\"is_blank\"] == \"False\")\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for LDA: Latent Dirichlet Allocation\n",
    "# from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"lemm_text\", outputCol=\"words\")\n",
    "#data = tokenizer.transform(data)\n",
    "vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "#idfModel = idf.fit(data)\n",
    "\n",
    "lda = LDA(k=20, seed=1, optimizer=\"em\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer,idf, lda])\n",
    "\n",
    "\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic terms\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return udf(termsIdx2Term, ArrayType(StringType()))\n",
    "\n",
    "vectorizerModel = model.stages[1]\n",
    "vocabList = vectorizerModel.vocabulary\n",
    "final = ldatopics.withColumn(\"Terms\", termsIdx2Term(vocabList)(\"termIndices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
